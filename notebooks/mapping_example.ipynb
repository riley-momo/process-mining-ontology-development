{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import os\n",
    "import yatter\n",
    "from ruamel.yaml import YAML\n",
    "import kglab\n",
    "import re\n",
    "from string import Template\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define paths\n",
    "data_dir = '../data'\n",
    "output_dir = '../output'\n",
    "config_dir = '../config'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "mapping_file = data_dir + '/sample_mapping.yaml'\n",
    "rml_output_path = output_dir + '/sample_mapping_rml.ttl'\n",
    "kg_config_path = config_dir + '/sample_kg_config.ini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define KGLab config\n",
    "config = f\"\"\"\n",
    "[test]\n",
    "mappings={rml_output_path}\n",
    "\"\"\"\n",
    "with open(kg_config_path, 'w') as f:\n",
    "    f.write(config)\n",
    "# define KG namespaces   \n",
    "namespaces = {\n",
    "    'ex:' : \"http://example.com/\",\n",
    "    'on:' : \"https://stl.mie.utoronto.ca/ontologies/spm/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_log(log_path):\n",
    "  \"\"\"\n",
    "  Return a dataframe from a given XES log filepath or CSV\n",
    "  \"\"\"\n",
    "  if any(log_path.lower().endswith(ext) for ext in ['.xes', '.xes.gz']):\n",
    "    log = pm4py.read_xes(log_path)\n",
    "    df = pm4py.convert_to_dataframe(log)\n",
    "  elif log_path.lower().endswith('.csv'):\n",
    "    df = pd.read_csv(log_path)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseID</th>\n",
       "      <th>activityID</th>\n",
       "      <th>eventID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>resourceID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case_0</td>\n",
       "      <td>activity_A</td>\n",
       "      <td>event_0</td>\n",
       "      <td>2016-01-01 09:00:00.000000+00:00</td>\n",
       "      <td>user_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case_0</td>\n",
       "      <td>activity_A</td>\n",
       "      <td>event_1</td>\n",
       "      <td>2016-01-01 09:15:00.000000+00:00</td>\n",
       "      <td>user_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case_0</td>\n",
       "      <td>activity_C</td>\n",
       "      <td>event_2</td>\n",
       "      <td>2016-01-01 09:35:00.000000+00:00</td>\n",
       "      <td>user_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case_1</td>\n",
       "      <td>activity_A</td>\n",
       "      <td>event_3</td>\n",
       "      <td>2016-01-02 09:00:00.000000+00:00</td>\n",
       "      <td>user_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case_1</td>\n",
       "      <td>activity_B</td>\n",
       "      <td>event_4</td>\n",
       "      <td>2016-01-02 09:00:00.000000+00:00</td>\n",
       "      <td>user_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caseID  activityID  eventID                         timestamp resourceID\n",
       "0  case_0  activity_A  event_0  2016-01-01 09:00:00.000000+00:00     user_1\n",
       "1  case_0  activity_A  event_1  2016-01-01 09:15:00.000000+00:00     user_1\n",
       "2  case_0  activity_C  event_2  2016-01-01 09:35:00.000000+00:00     user_1\n",
       "3  case_1  activity_A  event_3  2016-01-02 09:00:00.000000+00:00     user_1\n",
       "4  case_1  activity_B  event_4  2016-01-02 09:00:00.000000+00:00     user_0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the sample event log\n",
    "log_df = load_df_from_log('../data/sample_log.csv')\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 22:13:32,536 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-07-30 22:13:32,537 | INFO: RML content is created!\n",
      "2024-07-30 22:13:32,545 | INFO: Mapping has been syntactically validated.\n",
      "2024-07-30 22:13:32,546 | INFO: Translation has finished successfully.\n"
     ]
    }
   ],
   "source": [
    "## convert YARRRML to RML\n",
    "yaml = YAML(typ='safe', pure=True)\n",
    "yarrrml_content = yaml.load(open(mapping_file))\n",
    "rml_content = yatter.translate(yarrrml_content)\n",
    "rml_file = open(rml_output_path, 'w')\n",
    "rml_file.write(rml_content)\n",
    "rml_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 22:13:33,702 | DEBUG: CONFIGURATION: {'output_file': 'knowledge-graph', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_dir': '', 'output_format': 'N-TRIPLES', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '24'}\n",
      "2024-07-30 22:13:33,703 | DEBUG: DATA SOURCE `test`: {'mappings': '../output/sample_mapping_rml.ttl'}\n",
      "2024-07-30 22:13:34,260 | INFO: 8 mapping rules retrieved.\n",
      "2024-07-30 22:13:34,269 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-07-30 22:13:34,274 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-07-30 22:13:34,278 | INFO: Mapping partition with 8 groups generated.\n",
      "2024-07-30 22:13:34,280 | INFO: Maximum number of rules within mapping group: 1.\n",
      "2024-07-30 22:13:34,281 | INFO: Mappings processed in 0.573 seconds.\n",
      "2024-07-30 22:13:34,285 | DEBUG: Parallelizing with 24 cores.\n",
      "2024-07-30 22:13:34,609 | INFO: Number of triples generated in total: 60.\n"
     ]
    }
   ],
   "source": [
    "# init knowledge graph\n",
    "kg = kglab.KnowledgeGraph(name=\"test\", namespaces=namespaces)\n",
    "# create instances from mapping\n",
    "kg.materialize(kg_config_path)\n",
    "# save rdf instances\n",
    "kg.save_rdf(output_dir + '/sample_log_instances.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert from RDF A-Box to PSL A-Box\n",
    "ABox = np.array([])\n",
    "\n",
    "## Query1: Simple unary predicates\n",
    "df = kg.query_as_df(sparql=\"SELECT ?s ?o WHERE {?s a ?o}\")\n",
    "unary_preds = df.apply(lambda x: re.sub(r'.*:', '', x['o']) + '(' + re.sub(r'.*/|>$', '', x['s']) + ')', axis=1).values\n",
    "ABox = np.concatenate((ABox, unary_preds), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query 2: Timepoints\n",
    "df = kg.query_as_df(sparql=\"SELECT ?s ?t WHERE {?s ns1:hasRecordedTime ?t}\")\n",
    "unique_timestamps = df['t'].unique()\n",
    "\n",
    "# create timestamp mapping\n",
    "timestamp_mapping = {timestamp: f'ts_{i}' for i, timestamp in enumerate(sorted(unique_timestamps))}\n",
    "\n",
    "# apply mapping\n",
    "df['new_t'] = df['t'].map(timestamp_mapping)\n",
    "\n",
    "# create ordering relations over timestamps\n",
    "unique_mapped_timestamps = sorted(df['new_t'].unique())\n",
    "timestamp_pairs = [(unique_mapped_timestamps[i], unique_mapped_timestamps[i+1]) for i in range(len(unique_mapped_timestamps) - 1)]\n",
    "\n",
    "before_relations = [f'before({t1},{t2})' for t1, t2 in timestamp_pairs]\n",
    "\n",
    "timestamp_preds = [f'timepoint({t})' for t in unique_mapped_timestamps]\n",
    "\n",
    "event_timings = df.apply(lambda x: 'hasRecordedTime({}, {})'.format(re.sub(r\".*/|>$\", '', x[\"s\"]), x[\"new_t\"]), axis=1).values\n",
    "\n",
    "ABox = np.concatenate((ABox, timestamp_preds, event_timings, before_relations), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query 3: Other binary relations\n",
    "df = kg.query_as_df(sparql=\"SELECT ?s ?p ?o WHERE {?s ?p ?o . FILTER (?p != rdf:type && ?p != ns1:hasRecordedTime)}\")\n",
    "binary_relations = df.apply(lambda x: f'{re.sub(r\".*:\", \"\", x[\"p\"])}({re.sub(r\".*/|>$\", \"\", x[\"s\"])}, {re.sub(r\".*/|>$\", \"\", x[\"o\"])})', axis=1).values\n",
    "\n",
    "ABox = np.concatenate((ABox, binary_relations), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lastly, add process instance relations\n",
    "df = kg.query_as_df(sparql=\"SELECT ?s WHERE {?s a ns1:Case}\")\n",
    "process_instance = df.apply(lambda x: f'hasProcess({re.sub(r\".*/|>$\", \"\", x[\"s\"])}, P1)', axis=1).values\n",
    "process_instance\n",
    "\n",
    "ABox = np.concatenate((ABox, process_instance), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Resource(user_0)', 'Resource(user_1)', 'Resource(user_2)',\n",
       "       'Event(event_3)', 'Event(event_6)', 'Event(event_1)',\n",
       "       'Event(event_2)', 'Event(event_9)', 'Event(event_7)',\n",
       "       'Event(event_4)', 'Event(event_5)', 'Event(event_8)',\n",
       "       'Event(event_0)', 'Case(case_0)', 'Case(case_2)', 'Case(case_1)',\n",
       "       'Activity(activity_C)', 'Activity(activity_B)',\n",
       "       'Activity(activity_A)', 'Activity(activity_D)', 'timepoint(ts_0)',\n",
       "       'timepoint(ts_1)', 'timepoint(ts_2)', 'timepoint(ts_3)',\n",
       "       'timepoint(ts_4)', 'timepoint(ts_5)', 'timepoint(ts_6)',\n",
       "       'hasRecordedTime(event_3, ts_4)', 'hasRecordedTime(event_4, ts_4)',\n",
       "       'hasRecordedTime(event_8, ts_2)', 'hasRecordedTime(event_1, ts_2)',\n",
       "       'hasRecordedTime(event_2, ts_3)', 'hasRecordedTime(event_7, ts_1)',\n",
       "       'hasRecordedTime(event_6, ts_0)', 'hasRecordedTime(event_0, ts_0)',\n",
       "       'hasRecordedTime(event_5, ts_5)', 'hasRecordedTime(event_9, ts_6)',\n",
       "       'before(ts_0,ts_1)', 'before(ts_1,ts_2)', 'before(ts_2,ts_3)',\n",
       "       'before(ts_3,ts_4)', 'before(ts_4,ts_5)', 'before(ts_5,ts_6)',\n",
       "       'hasCase(event_4, case_1)', 'hasActivity(event_5, activity_C)',\n",
       "       'hasResource(event_7, user_1)', 'hasActivity(event_3, activity_A)',\n",
       "       'hasActivity(event_7, activity_B)',\n",
       "       'hasActivity(event_8, activity_C)', 'hasCase(event_5, case_1)',\n",
       "       'hasCase(event_0, case_0)', 'hasResource(event_5, user_0)',\n",
       "       'hasResource(event_8, user_2)', 'hasResource(event_3, user_1)',\n",
       "       'hasCase(event_6, case_2)', 'hasCase(event_3, case_1)',\n",
       "       'hasResource(event_0, user_1)', 'hasCase(event_2, case_0)',\n",
       "       'hasResource(event_4, user_0)', 'hasCase(event_9, case_2)',\n",
       "       'hasActivity(event_0, activity_A)', 'hasResource(event_2, user_1)',\n",
       "       'hasCase(event_8, case_2)', 'hasCase(event_1, case_0)',\n",
       "       'hasCase(event_7, case_2)', 'hasResource(event_6, user_1)',\n",
       "       'hasResource(event_1, user_1)', 'hasResource(event_9, user_1)',\n",
       "       'hasActivity(event_4, activity_B)',\n",
       "       'hasActivity(event_1, activity_A)',\n",
       "       'hasActivity(event_2, activity_C)',\n",
       "       'hasActivity(event_6, activity_A)',\n",
       "       'hasActivity(event_9, activity_D)', 'hasProcess(case_0, P1)',\n",
       "       'hasProcess(case_2, P1)', 'hasProcess(case_1, P1)'], dtype=object)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ABox to file\n",
    "with open(output_dir + '/sample_log_ABox.p9', 'w') as f:\n",
    "    for item in ABox:\n",
    "        f.write(\"%s.\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Oriented Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogProcessor:\n",
    "    def __init__(self, log_path,\n",
    "                 column_dict={'case_id': 'case:concept:name', 'activity': 'concept:name', 'timestamp': 'time:timestamp', 'resource': 'org:resource', 'event_id' : None},\n",
    "                 prefixes={'ex':'http://www.example.com/', 'on': 'https://stl.mie.utoronto.ca/ontologies/spm/'},\n",
    "                 process_name='P1'):\n",
    "        self.process_name = process_name\n",
    "        self.column_dict = column_dict\n",
    "        self.prefixes = prefixes\n",
    "        self.log_path = log_path\n",
    "        self.log_df = self.load_df_from_log()\n",
    "        self.mapping = self.build_mapping()\n",
    "        self.fol_abox = np.array([])\n",
    "\n",
    "    \n",
    "    def load_df_from_log(self):\n",
    "        \"\"\"\n",
    "        Return a dataframe from a given XES log filepath or CSV \n",
    "        and creates a temporary csv file with additional columns needed for consistent processing\n",
    "        \"\"\"\n",
    "        log_path = self.log_path\n",
    "        col_dict = self.column_dict\n",
    "        if any(log_path.lower().endswith(ext) for ext in ['.xes', '.xes.gz']): # if log is in XES format\n",
    "            log = pm4py.read_xes(log_path)\n",
    "            df = pm4py.convert_to_dataframe(log)\n",
    "            #df.to_csv(re.sub(r'\\.xes(\\.gz)?$', '.csv', log_path), index=False)\n",
    "        elif log_path.lower().endswith('.csv'): # if log is in CSV format\n",
    "            df = pd.read_csv(log_path)\n",
    "        else: # if log is in an unsupported format\n",
    "            return None\n",
    "\n",
    "        # add a process instance column to the dataframe\n",
    "        df['processID'] = self.process_name\n",
    "        \n",
    "        # ensure non-overlapping URIs by prefixing columns with letter type encoding\n",
    "        if not col_dict['event_id']: # if no unique identifier for events, create one\n",
    "            df['event_id'] = df.index\n",
    "            self.column_dict['event_id'] = 'event_id'\n",
    "        df[col_dict['event_id']] = df[col_dict['event_id']].apply(lambda x: f'E{str(x)}')\n",
    "        df[col_dict['case_id']] = df[col_dict['case_id']].apply(lambda x: f'C{str(x)}')\n",
    "        df[col_dict['activity']] = df[col_dict['activity']].apply(lambda x: f'A{str(x)}')\n",
    "        df[col_dict['resource']] = df[col_dict['resource']].apply(lambda x: f'R{str(x)}')\n",
    "        \n",
    "        # create temporary log csv\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as f:\n",
    "            f.write(df.to_csv(index=False))\n",
    "            self.log_path = f.name\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def build_mapping(self):\n",
    "        \"\"\"\n",
    "        Modifies YARRML mapping according to expected columns in the log\n",
    "        \"\"\"\n",
    "        \n",
    "        mapping_template = \"\"\"\n",
    "        prefixes:\n",
    "            ex: $ex_prefix\n",
    "            on: $on_prefix\n",
    "\n",
    "        mappings:\n",
    "            events:\n",
    "                sources:\n",
    "                - ['$log_path~$log_format']\n",
    "                s: ex:$$($eventID)\n",
    "                po:\n",
    "                - [a, on:Event]\n",
    "                - [on:hasCase, ex:$$($caseID)]\n",
    "                - [on:hasActivity, ex:$$($activityID)]\n",
    "                - [on:hasResource, ex:$$($resourceID)]\n",
    "                - [on:hasRecordedTime, $$($timestamp), xsd:dateTimeStamp]\n",
    "\n",
    "            resources:\n",
    "                sources:\n",
    "                - ['$log_path~$log_format']\n",
    "                s: ex:$$($resourceID)\n",
    "                po:\n",
    "                - [a, on:Resource]\n",
    "\n",
    "            cases:\n",
    "                sources:\n",
    "                - ['$log_path~$log_format']\n",
    "                s: ex:$$($caseID)\n",
    "                po:\n",
    "                - [a, on:Case]\n",
    "                - [on:hasProcess, ex:$$(processID)]\n",
    "\n",
    "            activities:\n",
    "                sources:\n",
    "                - ['$log_path~$log_format']\n",
    "                s: ex:$$($activityID)\n",
    "                po:\n",
    "                - [a, on:Activity]\n",
    "        \"\"\"\n",
    "        mapping_template = Template(mapping_template)\n",
    "        mapping_string = mapping_template.substitute(\n",
    "            log_path=self.log_path,\n",
    "            log_format='csv',\n",
    "            ex_prefix=self.prefixes['ex'],\n",
    "            on_prefix=self.prefixes['on'],\n",
    "            eventID=self.column_dict['event_id'],\n",
    "            caseID=self.column_dict['case_id'],\n",
    "            activityID=self.column_dict['activity'],\n",
    "            resourceID=self.column_dict['resource'],\n",
    "            timestamp=self.column_dict['timestamp']\n",
    "        )\n",
    "        yaml = YAML(typ='safe', pure=True)\n",
    "        yarrrml_content = yaml.load(mapping_string)\n",
    "        rml_mapping = yatter.translate(yarrrml_content)\n",
    "        # write rml mapping to temporary file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.ttl') as f:\n",
    "            f.write(rml_mapping)\n",
    "            self.rml_path = f.name\n",
    "        \n",
    "        return rml_mapping\n",
    "    \n",
    "    def generate_knowledge_graph(self):\n",
    "        \"\"\"\n",
    "        Generates a knowledge graph from the log and mapping\n",
    "        \"\"\"\n",
    "        # init knowledge graph\n",
    "        kg = kglab.KnowledgeGraph(name=\"test\", namespaces=self.prefixes)\n",
    "        # generate config\n",
    "        config_string = f\"\"\"\n",
    "        [{self.process_name}]\n",
    "        mappings={self.rml_path}\n",
    "        \"\"\"\n",
    "        # write config to temporary file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.ini') as f:\n",
    "            f.write(config_string)\n",
    "            kg_config_path = f.name\n",
    "        # generate the knowledge graph    \n",
    "        kg.materialize(kg_config_path)\n",
    "        \n",
    "        self.kg = kg\n",
    "        \n",
    "        return kg\n",
    "        \n",
    "    def save_knowledge_graph(self, output_path):\n",
    "        if not hasattr(self, 'kg'):\n",
    "            self.generate_knowledge_graph()\n",
    "        output_file = output_path + f'{self.process_name}_log_instances.ttl'\n",
    "        self.kg.save_rdf(output_file)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def generate_FOL(self):\n",
    "        \"\"\"\n",
    "        Generates a First Order Logic representation of the log\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'kg'):\n",
    "            self.generate_knowledge_graph()\n",
    "            \n",
    "        # helper functions definitions for converting RDF to FOL    \n",
    "        def query_and_apply(query, func):\n",
    "            df = self.kg.query_as_df(sparql=query)\n",
    "            vals = df.apply(func, axis=1).values\n",
    "            self.fol_abox = np.concatenate((self.fol_abox, vals), axis=0)\n",
    "\n",
    "        strip_ex_prefix  = lambda x: re.sub(r\".*/|>$\", '', x)\n",
    "        strip_on_prefix = lambda x: re.sub(r\".*:\", '', x)\n",
    "        unary_pred = lambda s,o : f'{strip_on_prefix(o)}({strip_ex_prefix(s)})'\n",
    "        binary_pred = lambda s,p,o : f'{strip_on_prefix(p)}({strip_ex_prefix(s)}, {strip_ex_prefix(o)})'\n",
    "        \n",
    "        # helper function for converting timepoints from data property to FOL\n",
    "        def convert_timepoints(kg):\n",
    "            tp_query = \"SELECT ?s ?t WHERE {?s ns1:hasRecordedTime ?t}\"\n",
    "            df = kg.query_as_df(sparql=tp_query)\n",
    "            unique_timestamps = df['t'].unique()\n",
    "            # create timestamp mapping\n",
    "            timestamp_mapping = {timestamp: f'ts_{i}' for i, timestamp in enumerate(sorted(unique_timestamps))}\n",
    "            # apply mapping\n",
    "            df['new_t'] = df['t'].map(timestamp_mapping)\n",
    "            # create ordering relations over timestamps\n",
    "            unique_mapped_timestamps = sorted(df['new_t'].unique())\n",
    "            timestamp_pairs = [(unique_mapped_timestamps[i], unique_mapped_timestamps[i+1]) for i in range(len(unique_mapped_timestamps) - 1)]\n",
    "            before_relations = [f'before({t1},{t2})' for t1, t2 in timestamp_pairs]\n",
    "            timestamp_preds = [f'timepoint({t})' for t in unique_mapped_timestamps]\n",
    "            event_timings = df.apply(lambda x: 'hasRecordedTime({}, {})'.format(re.sub(r\".*/|>$\", '', x[\"s\"]), x[\"new_t\"]), axis=1).values\n",
    "            # add to Abox\n",
    "            self.fol_abox = np.concatenate((self.fol_abox, timestamp_preds, event_timings, before_relations), axis=0)\n",
    "        \n",
    "        \n",
    "        # Convert simple unary rdf:type predicates\n",
    "        type_query = \"SELECT ?s ?o WHERE {?s a ?o}\"\n",
    "        type_f = lambda x: unary_pred(x['s'], x['o'])\n",
    "        query_and_apply(type_query, type_f)\n",
    "        print('test')\n",
    "        \n",
    "        # convert binary relations other than time and rdf:type\n",
    "        relation_query = \"SELECT ?s ?p ?o WHERE {?s ?p ?o . FILTER (?p != rdf:type && ?p != ns1:hasRecordedTime)}\"\n",
    "        relation_f = lambda x: binary_pred(x['s'], x['p'], x['o'])\n",
    "        query_and_apply(relation_query, relation_f)\n",
    "        \n",
    "        # convert timepoints\n",
    "        convert_timepoints(self.kg)\n",
    "        \n",
    "        return self.fol_abox\n",
    "        \n",
    "    def save_FOL(self, output_dir, format='prover9'):\n",
    "        \n",
    "        if self.fol_abox.size == 0:\n",
    "            self.generate_FOL()\n",
    "        file_ext_map = {'prover9': '.p9', 'clif': '.clif'}\n",
    "        literal_map = {'prover9': lambda x: f'{str(x)}.\\n', 'clif': lambda x: f'({str(x)}\\n)'}\n",
    "        output_file = output_dir + f'{self.process_name}_log_literals{file_ext_map[format]}'\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            for item in self.fol_abox:\n",
    "                f.write(literal_map[format](item))\n",
    "                \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 22:19:37,233 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-07-30 22:19:37,234 | INFO: RML content is created!\n",
      "2024-07-30 22:19:37,242 | INFO: Mapping has been syntactically validated.\n",
      "2024-07-30 22:19:37,243 | INFO: Translation has finished successfully.\n",
      "2024-07-30 22:19:37,246 | DEBUG: CONFIGURATION: {'output_file': 'knowledge-graph', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_dir': '', 'output_format': 'N-TRIPLES', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '24'}\n",
      "2024-07-30 22:19:37,247 | DEBUG: DATA SOURCE `P1`: {'mappings': '/tmp/tmpl8mzwsue.ttl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 22:19:37,795 | INFO: 9 mapping rules retrieved.\n",
      "2024-07-30 22:19:37,803 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-07-30 22:19:37,808 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-07-30 22:19:37,811 | INFO: Mapping partition with 9 groups generated.\n",
      "2024-07-30 22:19:37,813 | INFO: Maximum number of rules within mapping group: 1.\n",
      "2024-07-30 22:19:37,814 | INFO: Mappings processed in 0.562 seconds.\n",
      "2024-07-30 22:19:37,818 | DEBUG: Parallelizing with 24 cores.\n",
      "2024-07-30 22:19:38,074 | INFO: Number of triples generated in total: 63.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "col_dict = {'case_id': 'caseID', 'activity': 'activityID', 'timestamp': 'timestamp', 'resource': 'resourceID', 'event_id' : 'eventID'}\n",
    "output_dir= '../output/testing/'\n",
    "namespaces = {'ex' : \"http://example.com/\", 'on' : \"https://stl.mie.utoronto.ca/ontologies/spm/\"}\n",
    "log_processor = LogProcessor('../data/sample_log.csv', process_name='P1', column_dict=col_dict, prefixes=namespaces)\n",
    "log_processor.save_knowledge_graph(output_dir)\n",
    "log_processor.save_FOL(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Resource(Ruser_1)', 'Resource(Ruser_2)', 'Resource(Ruser_0)',\n",
       "       'Event(Eevent_3)', 'Event(Eevent_8)', 'Event(Eevent_2)',\n",
       "       'Event(Eevent_4)', 'Event(Eevent_7)', 'Event(Eevent_1)',\n",
       "       'Event(Eevent_9)', 'Event(Eevent_5)', 'Event(Eevent_0)',\n",
       "       'Event(Eevent_6)', 'Case(Ccase_2)', 'Case(Ccase_1)',\n",
       "       'Case(Ccase_0)', 'Activity(Aactivity_B)', 'Activity(Aactivity_A)',\n",
       "       'Activity(Aactivity_D)', 'Activity(Aactivity_C)',\n",
       "       'hasActivity(Eevent_6, Aactivity_A)',\n",
       "       'hasActivity(Eevent_9, Aactivity_D)', 'hasCase(Eevent_7, Ccase_2)',\n",
       "       'hasActivity(Eevent_0, Aactivity_A)',\n",
       "       'hasResource(Eevent_6, Ruser_1)', 'hasCase(Eevent_1, Ccase_0)',\n",
       "       'hasResource(Eevent_0, Ruser_1)', 'hasResource(Eevent_2, Ruser_1)',\n",
       "       'hasCase(Eevent_3, Ccase_1)', 'hasActivity(Eevent_5, Aactivity_C)',\n",
       "       'hasCase(Eevent_5, Ccase_1)', 'hasProcess(Ccase_2, P1)',\n",
       "       'hasActivity(Eevent_7, Aactivity_B)', 'hasProcess(Ccase_1, P1)',\n",
       "       'hasResource(Eevent_9, Ruser_1)', 'hasCase(Eevent_4, Ccase_1)',\n",
       "       'hasResource(Eevent_1, Ruser_1)', 'hasCase(Eevent_6, Ccase_2)',\n",
       "       'hasActivity(Eevent_1, Aactivity_A)',\n",
       "       'hasActivity(Eevent_3, Aactivity_A)',\n",
       "       'hasResource(Eevent_7, Ruser_1)', 'hasResource(Eevent_5, Ruser_0)',\n",
       "       'hasCase(Eevent_8, Ccase_2)', 'hasCase(Eevent_2, Ccase_0)',\n",
       "       'hasCase(Eevent_9, Ccase_2)', 'hasResource(Eevent_3, Ruser_1)',\n",
       "       'hasActivity(Eevent_2, Aactivity_C)',\n",
       "       'hasActivity(Eevent_4, Aactivity_B)', 'hasCase(Eevent_0, Ccase_0)',\n",
       "       'hasActivity(Eevent_8, Aactivity_C)',\n",
       "       'hasResource(Eevent_4, Ruser_0)', 'hasResource(Eevent_8, Ruser_2)',\n",
       "       'hasProcess(Ccase_0, P1)', 'timepoint(ts_0)', 'timepoint(ts_1)',\n",
       "       'timepoint(ts_2)', 'timepoint(ts_3)', 'timepoint(ts_4)',\n",
       "       'timepoint(ts_5)', 'timepoint(ts_6)',\n",
       "       'hasRecordedTime(Eevent_8, ts_2)',\n",
       "       'hasRecordedTime(Eevent_1, ts_2)',\n",
       "       'hasRecordedTime(Eevent_0, ts_0)',\n",
       "       'hasRecordedTime(Eevent_6, ts_0)',\n",
       "       'hasRecordedTime(Eevent_7, ts_1)',\n",
       "       'hasRecordedTime(Eevent_5, ts_5)',\n",
       "       'hasRecordedTime(Eevent_3, ts_4)',\n",
       "       'hasRecordedTime(Eevent_4, ts_4)',\n",
       "       'hasRecordedTime(Eevent_2, ts_3)',\n",
       "       'hasRecordedTime(Eevent_9, ts_6)', 'before(ts_0,ts_1)',\n",
       "       'before(ts_1,ts_2)', 'before(ts_2,ts_3)', 'before(ts_3,ts_4)',\n",
       "       'before(ts_4,ts_5)', 'before(ts_5,ts_6)'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_processor.fol_abox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$(test)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_template = '$$($x)'\n",
    "test_template = Template(test_template)\n",
    "test_template.substitute(x='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
